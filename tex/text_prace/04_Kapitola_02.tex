\chapter{Topology without tears}
\graphicspath{ {/home/tomasp/Dokumenty/Master_Thesis/figures/} }
%%Definitions, notations, remarks and examples
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]
\newtheorem*{remark}{Remark}

Unfortunately, we cannot go deeper into TDA without establishing a basic topology dictionary and theoretical background. This chapter will be a brief summary and introduction to the necessary definitions and theorems we will use. We assume that the reader already has rudimentary understanding of metric and topological spaces but for the sake of establishing notation and terminology, we're going to go through them anyway. (For more details, see Appendix A?)

\section{Metric Spaces}

\begin{definition}
  A \textit{metric space} is a tuple $(X, \partial_{X})$, where $X$ is a set and $\partial_{X}: X \times X \to \mathbb{R}$ is a function satisfying the following:
  \begin{enumerate}[label=\arabic*)]
    \centering
    \item $\partial_{X}(x,y) = 0 \iff x = y.$
    \item $\forall x,y \in X, \quad \partial_{X}(x,y) = \partial_{X}(y,x).$
    \item $\forall x,y,z \in X, \quad \partial_{X}(x,z) \leq \partial_{X}(x,y) + \partial_{X}(y,z).$
  \end{enumerate}
\end{definition}
The last property is known as the \textit{triangle inequality}.

\begin{remark}
Later on, we'll encounter \textit{dissimilarity measures}, i.e., metrics that fail to satisfy one of the three points in the definition above. The most prominent of these will be the Kullback-Leibler divergence and the Gromov-Hausdorff distance.
\end{remark}

Typical examples of known metrics include:

\begin{example}
  On Euclidean spaces $\mathbb{R}^{n}$, for two $n$-tuples we have the standard distance metric given by
  \begin{equation*}
    \partial_{\mathbb{R}^{n}}((x_{1}, \ldots, x_{n}),(y_{1}, \ldots, y_{n})) = \sqrt{(x_{1}-y_{1})^{2} + \ldots (x_{n} - y_{n})^{2}}.
  \end{equation*}
\end{example}

\begin{example}
  Fix an alphabet $\Sigma$. Let $x$ and $y$ be words of length $n$ with letters in $\Sigma$. The \textit{Hamming distance} is then defined as the number of positions at which the letters of $x$ and $y$ are different:
  \begin{equation*}
    \partial_{H}(x,y) = #\{i \: \vert \: x_{i} \ne y_{i}\}.
  \end{equation*}
\end{example}

Another important notion that we'll use are \textit{open} and \textit{closed} balls of radius $\varepsilon$ and center $x \in X$:
\begin{equation*}
  B_{\varepsilon}(x) = \{z \in X \: \vert \: \partial_{X}(z,x) < \varepsilon \} \quad \text{and} \quad \overline{B}_{\varepsilon}(x) = \{z \in X \: \vert \: \partial_{X}(z,x) \leq \varepsilon \}
\end{equation}
